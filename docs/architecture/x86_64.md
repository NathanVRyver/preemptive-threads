# x86_64 Architecture Guide

This guide covers x86_64-specific features and optimizations in the preemptive multithreading library.

## Architecture Overview

The x86_64 implementation provides the most mature and feature-complete support, including:

- **Full register context switching** with FPU/SSE/AVX state
- **APIC timer integration** for hardware-driven preemption  
- **Optimized assembly routines** for critical paths
- **Advanced security features** including CET, MPX, and Intel CFI
- **Performance counters** and PMU integration
- **NUMA awareness** for multi-socket systems

## Context Switching

### Register Set

The x86_64 context switch saves the complete architectural state:

```rust
#[repr(C)]
pub struct X86_64Context {
    // General purpose registers
    pub rax: u64, pub rbx: u64, pub rcx: u64, pub rdx: u64,
    pub rsi: u64, pub rdi: u64, pub rbp: u64, pub rsp: u64,
    pub r8: u64,  pub r9: u64,  pub r10: u64, pub r11: u64,
    pub r12: u64, pub r13: u64, pub r14: u64, pub r15: u64,
    
    // Status and control
    pub rflags: u64,
    pub rip: u64,
    
    // Floating point state (512 bytes for FXSAVE)
    pub fpu_state: [u8; 512],
    
    // Extended state (AVX, etc.)
    pub extended_state: Option<ExtendedState>,
}
```

### Assembly Implementation

The core context switch uses optimized assembly:

```asm
// Save current context
pushfq                  // Save flags
movq %rax, 0(%rdi)     // Save RAX
movq %rbx, 8(%rdi)     // Save RBX
...
fxsave 128(%rdi)       // Save FPU/SSE state

// Load new context  
fxrstor 128(%rsi)      // Restore FPU/SSE state
movq 0(%rsi), %rax     // Restore RAX
movq 8(%rsi), %rbx     // Restore RBX
...
popfq                  // Restore flags
ret                    // Switch to new thread
```

### Performance Characteristics

| Operation | Cycles | Time (3GHz) | Notes |
|-----------|---------|-------------|-------|
| Basic context switch | 200-300 | ~100ns | Minimal register set |
| Full context switch | 400-600 | ~200ns | Including FPU state |
| AVX context switch | 800-1200 | ~400ns | With extended state |

## Timer Integration

### APIC Timer

The x86_64 implementation uses the Local APIC timer for preemption:

```rust
// Initialize APIC timer for 10ms intervals
pub fn init_apic_timer(interval_ms: u32) -> Result<(), TimerError> {
    let apic = LocalApic::new();
    
    // Configure timer in periodic mode
    apic.set_timer_mode(TimerMode::Periodic);
    apic.set_timer_divide(TimerDivide::By1);
    
    // Calculate initial count for desired interval
    let bus_frequency = measure_apic_frequency()?;
    let initial_count = (bus_frequency * interval_ms) / 1000;
    
    apic.set_initial_count(initial_count);
    apic.enable_timer_interrupt(TIMER_VECTOR);
    
    println!("APIC timer initialized: {}MHz, {}ms intervals", 
             bus_frequency / 1000000, interval_ms);
    Ok(())
}
```

### TSC Calibration

High-precision timing uses the Time Stamp Counter:

```rust
// Calibrate TSC frequency
fn calibrate_tsc() -> Result<u64, TimerError> {
    let start_tsc = rdtsc();
    let start_time = read_pit_count();
    
    // Wait for PIT to count down
    busy_wait_pit(PIT_CALIBRATION_TICKS);
    
    let end_tsc = rdtsc();
    let end_time = read_pit_count();
    
    let pit_ticks = start_time - end_time;
    let tsc_ticks = end_tsc - start_tsc;
    
    // Calculate TSC frequency
    let tsc_freq = (tsc_ticks * PIT_FREQUENCY) / pit_ticks;
    
    println!("TSC frequency: {}MHz", tsc_freq / 1000000);
    Ok(tsc_freq)
}
```

## Memory Management

### Virtual Memory Layout

```
Virtual Address Space Layout (x86_64):
0x0000000000000000 - 0x00007FFFFFFFFFFF : User space (128TB)
0x0000800000000000 - 0xFFFF7FFFFFFFFFFF : Non-canonical (invalid)
0xFFFF800000000000 - 0xFFFFFFFFFFFFFFFF : Kernel space (128TB)

Thread Stack Layout:
High Address  ┌─────────────────┐
              │   Guard Page    │ ← Stack overflow protection
              ├─────────────────┤
              │                 │
              │   Thread Stack  │ ← Grows downward
              │                 │
              ├─────────────────┤
              │   Stack Canary  │ ← Overflow detection  
Low Address   └─────────────────┘
```

### Page Table Management

```rust
// Set up thread stack with guard pages
pub fn create_protected_stack(size: usize) -> Result<ProtectedStack, MemoryError> {
    let total_size = size + PAGE_SIZE; // Stack + guard page
    let base_addr = allocate_virtual_memory(total_size)?;
    
    // Map stack pages as RW
    for page in 0..(size / PAGE_SIZE) {
        let page_addr = base_addr + (page * PAGE_SIZE);
        map_page(page_addr, PageFlags::WRITABLE | PageFlags::USER)?;
    }
    
    // Guard page (no permissions = triggers page fault)
    let guard_addr = base_addr + size;
    map_page(guard_addr, PageFlags::empty())?;
    
    Ok(ProtectedStack::new(base_addr, size))
}
```

## Security Features

### Intel CET Support

Control-flow Enforcement Technology provides hardware CFI:

```rust
// Enable Intel CET if available
pub fn init_intel_cet() -> Result<(), SecurityError> {
    if !cpu_has_cet() {
        return Err(SecurityError::UnsupportedFeature("Intel CET"));
    }
    
    // Enable shadow stack
    let cr4 = read_cr4();
    write_cr4(cr4 | CR4_CET);
    
    // Configure shadow stack for current thread
    let ssp = allocate_shadow_stack(SHADOW_STACK_SIZE)?;
    write_msr(MSR_PL0_SSP, ssp as u64);
    
    // Enable in thread context
    let cet_flags = CET_IBT_EN | CET_SHSTK_EN;
    write_msr(MSR_S_CET, cet_flags);
    
    println!("Intel CET enabled (IBT + Shadow Stack)");
    Ok(())
}
```

### Memory Protection Keys

Intel MPX provides fine-grained memory protection:

```rust
// Set up memory protection for thread isolation
pub fn setup_thread_mpx(thread_id: ThreadId) -> Result<(), SecurityError> {
    let bounds_dir = allocate_bounds_directory()?;
    
    // Initialize thread's bounds table
    for i in 0..MAX_BOUNDS_ENTRIES {
        bounds_dir[i] = BoundsEntry::invalid();
    }
    
    // Set up bounds for thread stack
    let stack = get_thread_stack(thread_id);
    let bounds_entry = BoundsEntry::new(
        stack.base(), 
        stack.base() + stack.size()
    );
    
    bounds_dir[STACK_BOUNDS_INDEX] = bounds_entry;
    
    // Enable MPX for this thread
    let xsave_mask = read_xcr0();
    write_xcr0(xsave_mask | XCR0_BNDREGS | XCR0_BNDCSR);
    
    Ok(())
}
```

## Performance Optimizations

### CPU Cache Optimization

```rust
// Align scheduler data to cache lines
#[repr(align(64))] // x86_64 cache line size
pub struct X86_64SchedulerData {
    pub current_thread: AtomicPtr<Thread>,
    pub run_queue: LockFreeQueue<ThreadId>,
    pub priority_bitmap: AtomicU64,
    // ... other hot data
}

// Prefetch optimization for context switches
pub unsafe fn prefetch_thread_context(context: *const X86_64Context) {
    // Prefetch general registers
    prefetch_t0(context as *const u8);
    
    // Prefetch FPU state
    prefetch_t0((context as *const u8).offset(128));
    
    // Prefetch stack top
    let stack_top = (*context).rsp as *const u8;
    for i in 0..4 {
        prefetch_t1(stack_top.offset(-(i * 64))); // 4 cache lines
    }
}
```

### SIMD Acceleration

```rust
// Use AVX2 for bulk memory operations
pub unsafe fn fast_memory_copy_avx2(dst: *mut u8, src: *const u8, len: usize) {
    let mut offset = 0;
    
    // Process 32-byte chunks with AVX2
    while offset + 32 <= len {
        let chunk = _mm256_loadu_si256(src.add(offset) as *const __m256i);
        _mm256_storeu_si256(dst.add(offset) as *mut __m256i, chunk);
        offset += 32;
    }
    
    // Handle remainder
    while offset < len {
        *dst.add(offset) = *src.add(offset);
        offset += 1;
    }
}
```

### Branch Prediction Optimization

```rust
// Use likely/unlikely hints for better branch prediction
#[inline(always)]
pub fn schedule_next_thread() -> Option<ThreadId> {
    let priority_bitmap = SCHEDULER.priority_bitmap.load(Ordering::Acquire);
    
    if likely!(priority_bitmap != 0) {
        // Fast path: threads are ready to run
        let highest_priority = 63 - priority_bitmap.leading_zeros();
        SCHEDULER.dequeue_thread(highest_priority as u8)
    } else {
        // Slow path: no threads ready
        None
    }
}

// Compiler intrinsics for branch hints
#[inline(always)]
fn likely(b: bool) -> bool {
    unsafe { core::intrinsics::likely(b) }
}

#[inline(always)]  
fn unlikely(b: bool) -> bool {
    unsafe { core::intrinsics::unlikely(b) }
}
```

## NUMA Optimization

### Topology Detection

```rust
// Detect NUMA topology using CPUID
pub fn detect_numa_topology() -> NumaTopology {
    let mut topology = NumaTopology::new();
    
    // Get number of logical processors
    let logical_procs = get_logical_processor_count();
    
    for cpu in 0..logical_procs {
        let apic_id = get_apic_id(cpu);
        let numa_node = apic_id_to_numa_node(apic_id);
        
        topology.add_cpu_to_node(cpu, numa_node);
    }
    
    topology
}

// NUMA-aware memory allocation
pub fn numa_alloc_thread_stack(preferred_node: NumaNodeId, size: usize) -> Result<*mut u8, MemoryError> {
    // Try to allocate on preferred node first
    if let Ok(ptr) = allocate_on_node(preferred_node, size) {
        return Ok(ptr);
    }
    
    // Fall back to closest node
    let topology = get_numa_topology();
    for &node in topology.get_nearest_nodes(preferred_node) {
        if let Ok(ptr) = allocate_on_node(node, size) {
            return Ok(ptr);
        }
    }
    
    // Last resort: any node
    allocate_anywhere(size)
}
```

## Debugging and Profiling

### Hardware Performance Counters

```rust
// Initialize PMU for thread profiling
pub fn init_performance_monitoring() -> Result<(), ProfilingError> {
    // Enable performance monitoring
    let cr4 = read_cr4();
    write_cr4(cr4 | CR4_PCE);
    
    // Configure PMU counters
    setup_pmu_counter(0, PmuEvent::Cycles, PmuFlags::USER | PmuFlags::KERNEL)?;
    setup_pmu_counter(1, PmuEvent::Instructions, PmuFlags::USER | PmuFlags::KERNEL)?;
    setup_pmu_counter(2, PmuEvent::CacheMisses, PmuFlags::USER | PmuFlags::KERNEL)?;
    setup_pmu_counter(3, PmuEvent::BranchMispredicts, PmuFlags::USER | PmuFlags::KERNEL)?;
    
    println!("PMU initialized for thread profiling");
    Ok(())
}

// Sample performance counters
pub fn sample_thread_performance(thread_id: ThreadId) -> ThreadPerfSample {
    let cycles = read_pmu_counter(0);
    let instructions = read_pmu_counter(1);
    let cache_misses = read_pmu_counter(2);
    let branch_mispredicts = read_pmu_counter(3);
    
    ThreadPerfSample {
        thread_id,
        timestamp: rdtsc(),
        cycles,
        instructions,
        ipc: instructions as f64 / cycles.max(1) as f64,
        cache_miss_rate: cache_misses as f64 / instructions.max(1) as f64,
        branch_mispredict_rate: branch_mispredicts as f64 / instructions.max(1) as f64,
    }
}
```

### Stack Unwinding

```rust
// Walk stack frames for debugging
pub unsafe fn walk_stack_frames(mut rbp: u64, callback: impl Fn(u64, u64)) {
    let mut depth = 0;
    
    while rbp != 0 && depth < MAX_STACK_DEPTH {
        // Read return address and previous frame pointer
        let return_addr = *(rbp.wrapping_add(8) as *const u64);
        let prev_rbp = *(rbp as *const u64);
        
        callback(return_addr, rbp);
        
        // Validate frame pointer before following
        if !is_valid_stack_address(prev_rbp) || prev_rbp <= rbp {
            break;
        }
        
        rbp = prev_rbp;
        depth += 1;
    }
}
```

## Best Practices

### Thread Configuration

```rust
// Optimal thread configuration for x86_64
let thread = ThreadBuilder::new()
    .stack_size(2 * 1024 * 1024)  // 2MB for performance (huge pages)
    .cpu_affinity(get_cpu_mask_for_node(numa_node))
    .priority(10)
    .enable_fpu(true)             // Enable if using floating point
    .enable_avx(cpu_has_avx())    // Enable AVX if available
    .spawn(worker_function)?;
```

### Performance Tips

1. **Use huge pages** for thread stacks when possible (2MB pages)
2. **Pin threads to specific cores** to improve cache locality
3. **Enable FPU context switching** only when needed
4. **Use NUMA-aware allocation** for memory-intensive workloads
5. **Align hot data structures** to cache line boundaries (64 bytes)
6. **Minimize system calls** in critical paths
7. **Use CPU-specific optimizations** when available (AVX, etc.)

### Security Considerations

1. **Enable Intel CET** on supported processors
2. **Use SMEP/SMAP** to prevent kernel exploitation
3. **Enable ASLR** for all thread stacks
4. **Use stack canaries** to detect buffer overflows
5. **Implement shadow stacks** for return address protection
6. **Enable MPX bounds checking** for memory safety
7. **Use hardware RNG** for security-critical operations

## Troubleshooting

### Common Issues

**Context switch crashes:**
- Check FPU state alignment (512-byte boundary)
- Verify stack pointer validity
- Ensure proper interrupt handling

**Performance degradation:**
- Profile with PMU counters
- Check for cache misses
- Verify NUMA topology
- Monitor scheduler latency

**Timer issues:**
- Verify APIC initialization
- Check interrupt routing
- Calibrate TSC properly
- Handle spurious interrupts

### Debugging Tools

```bash
# Check CPU features
./cpu_features --detailed

# Monitor performance counters  
perf stat -e cycles,instructions,cache-misses ./your_program

# Analyze context switch overhead
./scheduler_profiler --context-switches

# Verify security features
./security_audit --x86-64 --verbose
```

## References

- [Intel 64 and IA-32 Architectures Software Developer's Manual](https://software.intel.com/content/www/us/en/develop/articles/intel-sdm.html)
- [Intel Control-flow Enforcement Technology Specification](https://software.intel.com/sites/default/files/managed/4d/2a/control-flow-enforcement-technology-preview.pdf)
- [AMD64 Architecture Programmer's Manual](https://www.amd.com/system/files/TechDocs/24593.pdf)
- [x86_64 ABI Documentation](https://gitlab.com/x86-psABIs/x86-64-ABI)